{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baf65d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "# Test for GPU or CPU devices\n",
    "print(\"Device: \\n\", tf.config.experimental.list_physical_devices())\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_built_with_cuda())\n",
    "\n",
    "# CLAHE Function for preprocessing\n",
    "def clahe_function(img):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(8,8))\n",
    "    lab_img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab_img)\n",
    "    clahe_img = clahe.apply(l)\n",
    "    updated_lab_img2 = cv2.merge((clahe_img, a, b))\n",
    "    CLAHE_img = cv2.cvtColor(updated_lab_img2, cv2.COLOR_LAB2BGR)\n",
    "    return CLAHE_img\n",
    "\n",
    "# Load images function\n",
    "def load_images_from_folder(folder_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_folders = os.listdir(folder_path)\n",
    "    \n",
    "    for label, class_folder in enumerate(class_folders):\n",
    "        class_folder_path = os.path.join(folder_path, class_folder)\n",
    "        for filename in os.listdir(class_folder_path):\n",
    "            img_path = os.path.join(class_folder_path, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = clahe_function(img)\n",
    "            img = cv2.resize(img, (224, 224))\n",
    "            img = img / 255.0\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load datasets\n",
    "train_folder = r\"E:/sabbir/train\"\n",
    "test_folder = r\"E:/sabbir/test\"\n",
    "valid_folder = r\"E:/sabbir/valid\"\n",
    "\n",
    "X_train, y_train = load_images_from_folder(train_folder)\n",
    "X_test, y_test = load_images_from_folder(test_folder)\n",
    "X_valid, y_valid = load_images_from_folder(valid_folder)\n",
    "\n",
    "# Merge all datasets\n",
    "X_all = np.concatenate((X_train, X_test, X_valid), axis=0)\n",
    "y_all = np.concatenate((y_train, y_test, y_valid), axis=0)\n",
    "\n",
    "# Split the merged dataset\n",
    "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X_all, y_all, test_size=0.20, stratify=y_all, random_state=0)\n",
    "\n",
    "# CNN Model\n",
    "model = tf.keras.Sequential()\n",
    "# 1st conv layer\n",
    "model.add(tf.keras.layers.Conv2D(32, 3, input_shape=(224, 224, 3)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2))\n",
    "# 2nd conv layer\n",
    "model.add(tf.keras.layers.Conv2D(64, 3, padding=\"valid\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2))\n",
    "# 3rd conv layer\n",
    "model.add(tf.keras.layers.Conv2D(128, 3, padding=\"valid\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2))\n",
    "# 4th conv layer\n",
    "model.add(tf.keras.layers.Conv2D(256, 3, padding=\"valid\"))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2))\n",
    "# Flatten Layer\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "# Dense Layer 1\n",
    "model.add(tf.keras.layers.Dense(8000))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(1024))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(512, name='feature_denseee'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "# Dense Layer 2\n",
    "model.add(tf.keras.layers.Dense(100))  # 100 Prominent Features are Extracted From This Layer\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "# Output Dense Layer\n",
    "model.add(tf.keras.layers.Dense(4))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=['acc'], optimizer=adam)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Train the CNN model\n",
    "history = model.fit(X_train_split, y_train_split, batch_size=32, epochs=100, verbose=1, validation_data=(X_test_split, y_test_split))\n",
    "\n",
    "# Save and reload the model\n",
    "model.save(r'E:/model2/save.h5')\n",
    "model = tf.keras.models.load_model(r'E:/model2/save.h5')\n",
    "\n",
    "# Create intermediate model for feature extraction\n",
    "intermediate_layer_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer('feature_denseee').output)\n",
    "intermediate_layer_model.summary()\n",
    "\n",
    "# Feature extraction\n",
    "feature_engg_data = intermediate_layer_model.predict(X_train_split)\n",
    "feature_engg_data = pd.DataFrame(feature_engg_data)\n",
    "\n",
    "# Save and load the features\n",
    "feature_engg_data.to_pickle(r'E:/model2/finalfeaturescovid.pkl')\n",
    "features = pd.read_pickle(r'E:/model2/finalfeaturescovid.pkl')\n",
    "\n",
    "# Normalize features\n",
    "x = feature_engg_data.values\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "# Apply SVD for dimensionality reduction\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "x_reduced = svd.fit_transform(x)\n",
    "\n",
    "# Split the reduced features into Training & Testing Set\n",
    "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(x_reduced, y_train_split, test_size=0.20, stratify=y_train_split, random_state=0)\n",
    "\n",
    "# Ensure the labels are in integer format\n",
    "def is_one_hot(encoded_labels):\n",
    "    return len(encoded_labels.shape) > 1 and encoded_labels.shape[1] > 1\n",
    "\n",
    "if is_one_hot(y_train_split):\n",
    "    y_train_split = np.argmax(y_train_split, axis=1)\n",
    "if is_one_hot(y_test_split):\n",
    "    y_test_split = np.argmax(y_test_split, axis=1)\n",
    "\n",
    "# Apply Machine Learning Algorithms for Classification\n",
    "# Gaussian Naive Bayes\n",
    "model_gnb = GaussianNB()\n",
    "model_gnb.fit(X_train_split, y_train_split)\n",
    "y_pred_gnb = model_gnb.predict_proba(X_test_split)\n",
    "\n",
    "# Support Vector Machine\n",
    "svclassifier = SVC(kernel='sigmoid', probability=True)\n",
    "svclassifier.fit(X_train_split, y_train_split)\n",
    "y_pred_svc = svclassifier.predict_proba(X_test_split)\n",
    "\n",
    "# Gradient Boosting Machine\n",
    "model_gbm = GradientBoostingClassifier(n_estimators=500, random_state=1)\n",
    "model_gbm.fit(X_train_split, y_train_split)\n",
    "y_pred_gbm = model_gbm.predict_proba(X_test_split)\n",
    "\n",
    "# k-Nearest Neighbors\n",
    "model_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "model_knn.fit(X_train_split, y_train_split)\n",
    "y_pred_knn = model_knn.predict_proba(X_test_split)\n",
    "\n",
    "# Random Forest Classifier\n",
    "RF = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "RF.fit(X_train_split, y_train_split)\n",
    "y_pred_rf = RF.predict_proba(X_test_split)\n",
    "\n",
    "# Ensemble Model\n",
    "voting_classifier = VotingClassifier(estimators=[('GNB', model_gnb), ('SVM', svclassifier), ('GBM', model_gbm), ('KNN', model_knn), ('RF', RF)], voting='soft')\n",
    "voting_classifier.fit(X_train_split, y_train_split)\n",
    "y_pred_vot = voting_classifier.predict_proba(X_test_split)\n",
    "\n",
    "# Binarize the output labels\n",
    "y_test_binarized = label_binarize(y_test_split, classes=[0, 1, 2, 3])\n",
    "\n",
    "# Dictionary to store ROC AUC for each model\n",
    "roc_auc = {}\n",
    "\n",
    "# Compute ROC curves and AUCs\n",
    "for model_name, y_pred_proba in {\n",
    "    \"GNB\": y_pred_gnb,\n",
    "    \"SVM\": y_pred_svc,\n",
    "    \"GBM\": y_pred_gbm,\n",
    "    \"KNN\": y_pred_knn,\n",
    "    \"RF\": y_pred_rf,\n",
    "    \"Ensemble\": y_pred_vot\n",
    "}.items():\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc[model_name] = dict()\n",
    "    \n",
    "    for i in range(4):  # Assuming 4 classes\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_pred_proba[:, i])\n",
    "        roc_auc[model_name][i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    plt.figure()\n",
    "    for i in range(4):  # Plotting ROC curves for each class\n",
    "        plt.plot(fpr[i], tpr[i], lw=2, label=f'Class {i} (area = {roc_auc[model_name][i]:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7330c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_gradcam_heatmap(model, img_array, class_index):\n",
    "    # Get the last convolutional layer\n",
    "    last_conv_layer = model.get_layer('conv2d_7')  # Adjust to your last conv layer's name\n",
    "    \n",
    "    # Create a model that outputs the last conv layer and the model's output\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        inputs=[model.input],\n",
    "        outputs=[last_conv_layer.output, model.output]\n",
    "    )\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        loss = predictions[:, class_index]\n",
    "    \n",
    "    # Compute gradients of the loss w.r.t. the last conv layer output\n",
    "    grads = tape.gradient(loss, conv_outputs)\n",
    "    \n",
    "    # Pool the gradients over all the axes\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    \n",
    "    # Get the values of the conv layer outputs and gradients\n",
    "    conv_outputs_value = conv_outputs[0].numpy()\n",
    "    pooled_grads_value = pooled_grads.numpy()\n",
    "    \n",
    "    # Apply the pooled gradients to the conv layer outputs\n",
    "    for i in range(pooled_grads_value.shape[-1]):\n",
    "        conv_outputs_value[:, :, i] *= pooled_grads_value[i]\n",
    "    \n",
    "    # Average the weighted conv layer outputs to get the heatmap\n",
    "    heatmap = np.mean(conv_outputs_value, axis=-1)\n",
    "    \n",
    "    # Normalize the heatmap\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "    \n",
    "    return heatmap\n",
    "\n",
    "def overlay_heatmap(heatmap, img, alpha=0.4):\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    superimposed_img = heatmap * alpha + img\n",
    "    return np.uint8(superimposed_img)\n",
    "\n",
    "# Select 4 sample images and their respective target classes\n",
    "sample_images = X_test[:4]  # Assuming X_test has at least 4 images\n",
    "sample_images_exp = np.expand_dims(sample_images, axis=-1)  # Add a dimension for batch\n",
    "\n",
    "# Display the original images and Grad-CAM heatmaps for 4 images\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "for i in range(4):\n",
    "    sample_image = sample_images[i]\n",
    "    sample_image_exp = np.expand_dims(sample_image, axis=0)  # Expand to match model input shape\n",
    "    class_index = np.argmax(model.predict(sample_image_exp))  # Class index for Grad-CAM\n",
    "\n",
    "    # Generate Grad-CAM heatmap\n",
    "    heatmap = generate_gradcam_heatmap(model, sample_image_exp, class_index)\n",
    "\n",
    "    # Convert image to original format\n",
    "    sample_image = sample_image * 255.0\n",
    "    sample_image = sample_image.astype(np.uint8)\n",
    "\n",
    "    # Overlay the heatmap on the image\n",
    "    overlayed_img = overlay_heatmap(heatmap, sample_image)\n",
    "\n",
    "    # Plot the original image and heatmap for each sample\n",
    "    plt.subplot(4, 2, i * 2 + 1)\n",
    "    plt.title(f'Original Image {i+1}')\n",
    "    plt.imshow(sample_image)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(4, 2, i * 2 + 2)\n",
    "    plt.title(f'Grad-CAM Heatmap {i+1}')\n",
    "    plt.imshow(overlayed_img)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
